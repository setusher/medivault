{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":2332307,"sourceType":"datasetVersion","datasetId":891819},{"sourceId":11210462,"sourceType":"datasetVersion","datasetId":7000107},{"sourceId":11210482,"sourceType":"datasetVersion","datasetId":7000118}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Background ü§¢\nTuberculosis, or TB, is a highly contagious disease caused by the bacteria 'Mycobacterium tuberculosis'. A deadly disease that affects the lungs and spreads to every other organ in the body, it can be spread through the air when someone speaks, coughs, or sneezes, and results in severe coughing, fever, pain, weight loss, night sweats, coughing up blood, and potentially death when left untreated. \n\nAs someone who knows people who have died from TB, I have created a convolutional neural network to accurately diagnose patients using chest X-rays, with user [Tawsifur Rahman](https://www.kaggle.com/tawsifurrahman)'s \"Tuberculosis (TB) Chest X-ray Database\" dataset, in an attempt to potentially save lives by preventing patients with TB from going untreated and gaining the symptoms mentioned above.\n\nThis dataset contains 3500 samples of chest X-rays of patients without tuberculosis, and 700 of patients with tuberculosis, that all look similar to the image below, which I will be using to train this CNN.\n\n![](https://storage.googleapis.com/kagglesdsdata/datasets/891819/2332307/TB_Chest_Radiography_Database/Normal/Normal-1019.png?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=databundle-worker-v2%40kaggle-161607.iam.gserviceaccount.com%2F20240713%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20240713T160233Z&X-Goog-Expires=345600&X-Goog-SignedHeaders=host&X-Goog-Signature=81e3c3b9b9fa4dfc6654ee819f45aa2a24fd64e87130250359a86f23723efcc10185744e2eb48faf4f82833472a62c4bb2488ed8452ad9bc8ae1e35d46b12e35e845e4ee5be597e3d326fae4b3a5b9e6c0afa958c0d405ae07f03ee0ed70b144d9f2859a8f37529cf2190f5d79df12587c1ad049075555cce2e0561289cacb6c4a9f1a20905092913a2879c439119184186d2f0dc115dd057413f36ec40fbdedaafdac108c0cc62ec31102803194787ac4897ba10359e3422f919932ff72e9e68a936b2c85add5b102f2490b7c4fb1991a4b2957943a0ac2e761c32ce32fef9c0eba91399b394fa799c73017847b9205231fa9f083875579a6da2cd890c31355)","metadata":{}},{"cell_type":"markdown","source":"# Image Processing üñºÔ∏è\nThe main problem with the data I wanted to fix was the class imbalance (3500 Normal images vs. 700 TB images).\nIn the code below I first read all the image files with OpenCV in and transformed them into a format suitable for SMOTE upsampling to even out the class counts.","metadata":{}},{"cell_type":"code","source":"#Importing the necessary libraries:\nimport cv2 as cv\nimport numpy as np\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.model_selection import train_test_split\nimport os","metadata":{"execution":{"iopub.status.busy":"2025-03-29T22:52:14.870057Z","iopub.execute_input":"2025-03-29T22:52:14.870518Z","iopub.status.idle":"2025-03-29T22:52:16.153220Z","shell.execute_reply.started":"2025-03-29T22:52:14.870480Z","shell.execute_reply":"2025-03-29T22:52:16.152537Z"},"trusted":true},"outputs":[],"execution_count":1},{"cell_type":"code","source":"#Initializing the values needed for all the image files\nnormaldir = '/kaggle/input/tuberculosis-tb-chest-xray-dataset/TB_Chest_Radiography_Database/Normal'\ntbdir = '/kaggle/input/tuberculosis-tb-chest-xray-dataset/TB_Chest_Radiography_Database/Tuberculosis'\nimages = []\nlabels = []\nimagesize = 256","metadata":{"execution":{"iopub.status.busy":"2025-03-29T22:52:16.154295Z","iopub.execute_input":"2025-03-29T22:52:16.154751Z","iopub.status.idle":"2025-03-29T22:52:16.158437Z","shell.execute_reply.started":"2025-03-29T22:52:16.154726Z","shell.execute_reply":"2025-03-29T22:52:16.157545Z"},"trusted":true},"outputs":[],"execution_count":2},{"cell_type":"code","source":"#Storing all the image directories in the 'images' array and corresponding them to either 1 for TB images or 0 for normal images.\nfor x in os.listdir(normaldir):\n    imagedir = os.path.join(normaldir, x)\n    image = cv.imread(imagedir, cv.IMREAD_GRAYSCALE)\n    image = cv.resize(image, (imagesize, imagesize))\n    images.append(image)\n    labels.append(0)\n    \nfor y in os.listdir(tbdir):\n    imagedir = os.path.join(tbdir, y)\n    image = cv.imread(imagedir, cv.IMREAD_GRAYSCALE)\n    image = cv.resize(image, (imagesize, imagesize))\n    images.append(image)\n    labels.append(1)","metadata":{"execution":{"iopub.status.busy":"2025-03-29T22:52:16.159862Z","iopub.execute_input":"2025-03-29T22:52:16.160145Z","iopub.status.idle":"2025-03-29T22:53:28.768637Z","shell.execute_reply.started":"2025-03-29T22:52:16.160107Z","shell.execute_reply":"2025-03-29T22:53:28.767664Z"},"trusted":true},"outputs":[],"execution_count":3},{"cell_type":"code","source":"#Converting to NumPy arrays since they have more features than regular lists\nimages = np.array(images)\nlabels = np.array(labels)\n\n#Splitting the images and labels into training and testing sets, then normalizing the values within them for computational efficiency (from 0-255 scale to 0-1 scale)\nimagetrain, imagetest, labeltrain, labeltest = train_test_split(images, labels, test_size=0.3, random_state=42)\nimagetrain = (imagetrain.astype('float32'))/255\nimagetest = (imagetest.astype('float32'))/255","metadata":{"execution":{"iopub.status.busy":"2025-03-29T22:53:28.769979Z","iopub.execute_input":"2025-03-29T22:53:28.770328Z","iopub.status.idle":"2025-03-29T22:53:29.474637Z","shell.execute_reply.started":"2025-03-29T22:53:28.770296Z","shell.execute_reply":"2025-03-29T22:53:29.473916Z"},"trusted":true},"outputs":[],"execution_count":4},{"cell_type":"code","source":"#Flattening the image array into 2D (making it [2940 images] x [all the pixels of the image in just one 1D array]) to be suitable for SMOTE oversampling\nimagetrain = imagetrain.reshape(2940, (imagesize*imagesize))\n\n#Performing oversampling\nsmote = SMOTE(random_state=42)\nimagetrain, labeltrain = smote.fit_resample(imagetrain, labeltrain)\n\n#Unflattening the images now to use them for convolutional neural network (4914 images of 256x256 size, with 1 color channel (grayscale, as compared to RGB with 3 color channels))\nimagetrain = imagetrain.reshape(-1, imagesize, imagesize, 1)\nprint(imagetrain.shape)","metadata":{"execution":{"iopub.status.busy":"2025-03-29T22:53:29.475452Z","iopub.execute_input":"2025-03-29T22:53:29.475692Z","iopub.status.idle":"2025-03-29T22:53:32.834187Z","shell.execute_reply.started":"2025-03-29T22:53:29.475671Z","shell.execute_reply":"2025-03-29T22:53:32.833394Z"},"trusted":true},"outputs":[{"name":"stdout","text":"(4914, 256, 256, 1)\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"#Classes balanced - equal counts of each label\nprint(np.unique(labeltrain, return_counts=True))","metadata":{"execution":{"iopub.status.busy":"2025-03-29T22:53:32.834982Z","iopub.execute_input":"2025-03-29T22:53:32.835313Z","iopub.status.idle":"2025-03-29T22:53:32.841529Z","shell.execute_reply.started":"2025-03-29T22:53:32.835282Z","shell.execute_reply":"2025-03-29T22:53:32.840669Z"},"trusted":true},"outputs":[{"name":"stdout","text":"(array([0, 1]), array([2457, 2457]))\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"# CNN Time üß†\nUsing Tensorflow's Sequential API for CNN modeling to diagnose all the patients in the testing set with a high accuracy.","metadata":{}},{"cell_type":"code","source":"#Importing the necessary libraries\nimport tensorflow as tf\nimport keras\nfrom keras import layers\nfrom keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout","metadata":{"execution":{"iopub.status.busy":"2025-03-29T22:53:32.842364Z","iopub.execute_input":"2025-03-29T22:53:32.842680Z","iopub.status.idle":"2025-03-29T22:53:45.150811Z","shell.execute_reply.started":"2025-03-29T22:53:32.842656Z","shell.execute_reply":"2025-03-29T22:53:45.149842Z"},"trusted":true},"outputs":[],"execution_count":7},{"cell_type":"code","source":"#The CNN model has 3 convolutional layers, each followed by pooling to summarize the features found by the layer, starting with 16 and multiplying by 2 each time for computational efficiency, as bits are structured in powers of 2. 3x3 filters and ReLU activation used.\ncnn = keras.Sequential(\n    [\n    #Input layer, same shape as all the images (256x256x1):\n    keras.Input(shape=(imagesize, imagesize, 1)),\n    \n    #1st convolutional layer:\n    Conv2D(16, (3, 3), activation='relu'),\n    MaxPooling2D((2, 2)),\n    \n    #2nd convolutional layer:\n    Conv2D(32, (3, 3), activation='relu'),\n    MaxPooling2D((2, 2)),\n    \n    #3rd convolutional layer:\n    Conv2D(64, (3, 3), activation='relu'),\n    MaxPooling2D((2, 2)),\n    \n    #Flattening layer for the dense layers:\n    Flatten(),\n    \n    #1st dense layer following the convolutional layers:\n    Dense(64, activation='relu'),\n    \n    #Dropout layer with heavy dropout rate to avoid overfitting in the large-ish dataset\n    Dropout(0.5),\n    \n    #Output layer that squeezes each image to either 0 or 1 with sigmoid activation\n    Dense(1, activation='sigmoid')\n    ]\n)","metadata":{"execution":{"iopub.status.busy":"2025-03-29T22:53:45.152567Z","iopub.execute_input":"2025-03-29T22:53:45.153042Z","iopub.status.idle":"2025-03-29T22:53:47.296867Z","shell.execute_reply.started":"2025-03-29T22:53:45.153019Z","shell.execute_reply":"2025-03-29T22:53:47.296209Z"},"trusted":true},"outputs":[],"execution_count":8},{"cell_type":"code","source":"#Compiling the model with parameters best suited for the task at hand:\ncnn.compile(\n    loss='binary_crossentropy', #Best for binary classification\n    optimizer = keras.optimizers.Adam(learning_rate=0.001), #Good starting LR for dataset of this size\n    metrics=['accuracy'], #Looking for accuracy\n)","metadata":{"execution":{"iopub.status.busy":"2025-03-29T22:53:47.297870Z","iopub.execute_input":"2025-03-29T22:53:47.298186Z","iopub.status.idle":"2025-03-29T22:53:47.309682Z","shell.execute_reply.started":"2025-03-29T22:53:47.298153Z","shell.execute_reply":"2025-03-29T22:53:47.308895Z"},"trusted":true},"outputs":[],"execution_count":9},{"cell_type":"code","source":"#Fitting the model, with the ReduceLROnPlateau callback added to it to reduce the learning rate to take smaller steps in increasing the accuracy whenever the learning rate plateaus (goes in the wrong direction)\n#Doing this with patience=1, meaning it will perform this if it even plateaus for one epoch, since only 10 epochs are used\n#factor=0.1 means that for every time the learning rate is reduced, it is reduced by a factor of 0.1 - it also won't go lower than 0.00001\nfrom keras.callbacks import ReduceLROnPlateau\nreduce_lr = ReduceLROnPlateau(monitor='accuracy', factor=0.1, patience=1, min_lr=0.00001, verbose=1)\n\n#Fitting the model w/ the callback. ON VS CODE, batch size of 16 makes each epoch take around a minute in this case w/ good accuracy, making the whole training process 10 min, but on Kaggle it should take longer due to less computational resources:\ncnn.fit(imagetrain, labeltrain, batch_size=16, epochs=10, verbose=2, callbacks = [reduce_lr])","metadata":{"execution":{"iopub.status.busy":"2025-03-29T22:53:47.310311Z","iopub.execute_input":"2025-03-29T22:53:47.310513Z","iopub.status.idle":"2025-03-29T22:54:26.346058Z","shell.execute_reply.started":"2025-03-29T22:53:47.310495Z","shell.execute_reply":"2025-03-29T22:54:26.345319Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Epoch 1/10\n308/308 - 10s - 32ms/step - accuracy: 0.8864 - loss: 0.2704 - learning_rate: 0.0010\nEpoch 2/10\n308/308 - 3s - 9ms/step - accuracy: 0.9571 - loss: 0.1177 - learning_rate: 0.0010\nEpoch 3/10\n308/308 - 3s - 9ms/step - accuracy: 0.9784 - loss: 0.0627 - learning_rate: 0.0010\nEpoch 4/10\n308/308 - 3s - 9ms/step - accuracy: 0.9825 - loss: 0.0531 - learning_rate: 0.0010\nEpoch 5/10\n308/308 - 3s - 9ms/step - accuracy: 0.9900 - loss: 0.0273 - learning_rate: 0.0010\nEpoch 6/10\n\nEpoch 6: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n308/308 - 3s - 9ms/step - accuracy: 0.9888 - loss: 0.0349 - learning_rate: 0.0010\nEpoch 7/10\n308/308 - 3s - 9ms/step - accuracy: 0.9949 - loss: 0.0159 - learning_rate: 1.0000e-04\nEpoch 8/10\n308/308 - 3s - 9ms/step - accuracy: 0.9969 - loss: 0.0095 - learning_rate: 1.0000e-04\nEpoch 9/10\n308/308 - 3s - 9ms/step - accuracy: 0.9980 - loss: 0.0073 - learning_rate: 1.0000e-04\nEpoch 10/10\n\nEpoch 10: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n308/308 - 3s - 9ms/step - accuracy: 0.9969 - loss: 0.0068 - learning_rate: 1.0000e-04\n","output_type":"stream"},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"<keras.src.callbacks.history.History at 0x7e1e2b69f640>"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"#Evaluating the data w/ multiple types of metrics\nprint('TESTING DATA:')\ncnn.evaluate(imagetest, labeltest, batch_size=32, verbose=2)\n\nprint('ADVANCED TESTING METRICS:')\nfrom sklearn.metrics import classification_report, confusion_matrix\npredictions = cnn.predict(imagetest, batch_size=32)\npredicted_labels = (predictions > 0.5).astype('int32')\nprint(classification_report(labeltest, predicted_labels))\nprint(confusion_matrix(labeltest, predicted_labels))","metadata":{"execution":{"iopub.status.busy":"2025-03-29T22:54:26.350745Z","iopub.execute_input":"2025-03-29T22:54:26.350975Z","iopub.status.idle":"2025-03-29T22:54:30.757844Z","shell.execute_reply.started":"2025-03-29T22:54:26.350956Z","shell.execute_reply":"2025-03-29T22:54:30.756885Z"},"trusted":true},"outputs":[{"name":"stdout","text":"TESTING DATA:\n40/40 - 2s - 47ms/step - accuracy: 0.9833 - loss: 0.1139\nADVANCED TESTING METRICS:\n\u001b[1m40/40\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step\n              precision    recall  f1-score   support\n\n           0       0.98      1.00      0.99      1043\n           1       0.98      0.92      0.95       217\n\n    accuracy                           0.98      1260\n   macro avg       0.98      0.96      0.97      1260\nweighted avg       0.98      0.98      0.98      1260\n\n[[1039    4]\n [  17  200]]\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"Very good accuracy and loss values, and great balance of precision between the 0 and 1 classes.","metadata":{}},{"cell_type":"code","source":"cnn.save(\"tb_model.keras\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T22:55:30.602899Z","iopub.execute_input":"2025-03-29T22:55:30.603286Z","iopub.status.idle":"2025-03-29T22:55:30.833060Z","shell.execute_reply.started":"2025-03-29T22:55:30.603258Z","shell.execute_reply":"2025-03-29T22:55:30.832347Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"# Conclusion ü§ì\nThough there are obviously better models out there for diagnosing TB from x-ray scans, this model can hopefully work as a starting point to show some of you guys how using a CNN for medical purposes works and how it can benefit our society. Hopefully one day, a 100% accurate TB diagnosis model can be made and properly implemented in the healthcare system so that deaths from TB can be avoided altogether.\n\n**But overall, thanks for reading this notebook - it means a lot. If you liked it please make sure to leave an upvote, and check out my other work as well!**","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.models import load_model\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\n\ndef predict_tumor(model, img_path, img_size=240):\n    \"\"\"\n    Loads and preprocesses an image, runs inference using the model,\n    and returns the predicted class and confidence.\n    \n    Assumes the model outputs a single probability (e.g., probability of TB).\n    If the probability is above 0.5, the image is predicted as \"TB Detected\";\n    otherwise \"Normal\".\n    \n    This version uses img_size=240 so that the flattened input has 240√ó240=57600 elements.\n    \"\"\"\n    # Load and resize the image in grayscale mode to match model training.\n    img = load_img(img_path, target_size=(img_size, img_size), color_mode=\"grayscale\")\n    # Convert the image to array and normalize pixel values to [0,1]\n    img_array = img_to_array(img) / 255.0\n    # Expand dims to create a batch of 1.\n    img_array = np.expand_dims(img_array, axis=0)\n    \n    # Debug: print the preprocessed image shape.\n    print(\"Preprocessed image shape:\", img_array.shape)\n    \n    # Get prediction probability from the model.\n    pred_prob = model.predict(img_array)[0][0]\n    \n    # Interpret the prediction.\n    if pred_prob > 0.5:\n        label = \"TB Detected\"\n        confidence = pred_prob\n    else:\n        label = \"Normal\"\n        confidence = 1 - pred_prob\n    \n    return label, confidence\n\n# --- Example Usage ---\n# Update these paths as needed:\nmodel_path = \"/kaggle/working/tb_model.keras\"  # Path to your saved model file.\nsample_img_path = \"/kaggle/input/cvjdvddvdvd/Tuberculosis-12.png\"  # Path to a sample image file.\n\n# Check that the files exist.\nif not os.path.isfile(model_path):\n    raise FileNotFoundError(f\"Model file not found: {model_path}\")\nif not os.path.isfile(sample_img_path):\n    raise FileNotFoundError(f\"Sample image not found: {sample_img_path}\")\n\n# Load the model.\nmodel = load_model(model_path)\nprint(\"Model loaded from:\", model_path)\n\n# Get prediction using img_size=240 to match the model's expected input.\npredicted_label, confidence = predict_tumor(model, sample_img_path, img_size=240)\nprint(f\"Prediction: {predicted_label} | Confidence: {confidence:.4f}\")\n\n# Display the image with prediction overlay.\nimg_disp = cv2.imread(sample_img_path)\n# Convert the image to grayscale for consistent display.\nimg_disp = cv2.cvtColor(img_disp, cv2.COLOR_BGR2GRAY)\nplt.imshow(img_disp, cmap='gray')\nplt.title(f\"Predicted: {predicted_label} | Confidence: {confidence:.4f}\")\nplt.axis(\"off\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T22:57:30.691249Z","iopub.execute_input":"2025-03-29T22:57:30.691587Z","iopub.status.idle":"2025-03-29T22:57:31.028045Z","shell.execute_reply.started":"2025-03-29T22:57:30.691565Z","shell.execute_reply":"2025-03-29T22:57:31.026842Z"}},"outputs":[{"name":"stdout","text":"Model loaded from: /kaggle/working/tb_model.keras\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-eaff698ef1f7>\u001b[0m in \u001b[0;36m<cell line: 53>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;31m# Get prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m \u001b[0mpredicted_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfidence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_tumor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_img_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Prediction: {predicted_label} | Confidence: {confidence:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-14-eaff698ef1f7>\u001b[0m in \u001b[0;36mpredict_tumor\u001b[0;34m(model, img_path, img_size)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m# Get prediction probability from the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mpred_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0;31m# Adjust the logic based on your model's output:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m# Here we assume a high probability indicates presence of TB.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/layers/input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    225\u001b[0m                     \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m                 }:\n\u001b[0;32m--> 227\u001b[0;31m                     raise ValueError(\n\u001b[0m\u001b[1;32m    228\u001b[0m                         \u001b[0;34mf'Input {input_index} of layer \"{layer_name}\" is '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m                         \u001b[0;34mf\"incompatible with the layer: expected axis {axis} \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Exception encountered when calling Sequential.call().\n\n\u001b[1mInput 0 of layer \"dense\" is incompatible with the layer: expected axis -1 of input shape to have value 57600, but received input with shape (1, 43264)\u001b[0m\n\nArguments received by Sequential.call():\n  ‚Ä¢ inputs=tf.Tensor(shape=(1, 224, 224, 1), dtype=float32)\n  ‚Ä¢ training=False\n  ‚Ä¢ mask=None"],"ename":"ValueError","evalue":"Exception encountered when calling Sequential.call().\n\n\u001b[1mInput 0 of layer \"dense\" is incompatible with the layer: expected axis -1 of input shape to have value 57600, but received input with shape (1, 43264)\u001b[0m\n\nArguments received by Sequential.call():\n  ‚Ä¢ inputs=tf.Tensor(shape=(1, 224, 224, 1), dtype=float32)\n  ‚Ä¢ training=False\n  ‚Ä¢ mask=None","output_type":"error"}],"execution_count":14},{"cell_type":"code","source":"import os\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.models import load_model\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\n\n# Load the model\nmodel_path = \"/kaggle/working/tb_model.keras\"\nif not os.path.isfile(model_path):\n    raise FileNotFoundError(f\"Model file not found: {model_path}\")\nmodel = load_model(model_path)\n\ndef predict_tb(model, img_path, img_size=24):\n    \"\"\"\n    Loads and preprocesses an image, then predicts using the TB detection model.\n    This function handles both CNN and dense input models.\n    \"\"\"\n    # Use the model's overall input shape instead of the first layer's\n    input_shape = model.input_shape  # e.g., (None, height, width, channels) or (None, features)\n    \n    # Determine target size and color mode based on input shape\n    if len(input_shape) == 2:\n        # Dense model expects flattened input: input_shape = (None, features)\n        needed_pixels = input_shape[1]\n        # Determine target size based on known cases:\n        if needed_pixels == 576:  # 24x24x1\n            target_size = 24\n            color_mode = \"grayscale\"\n        elif needed_pixels == 1728:  # 24x24x3\n            target_size = 24\n            color_mode = \"rgb\"\n        elif needed_pixels == 256:  # 16x16x1\n            target_size = 16\n            color_mode = \"grayscale\"\n        elif needed_pixels == 768:  # 16x16x3\n            target_size = 16\n            color_mode = \"rgb\"\n        else:\n            # Default: assume grayscale and derive target size\n            color_mode = \"grayscale\"\n            target_size = int(np.sqrt(needed_pixels))\n    else:\n        # Convolutional model expects multi-dimensional input: e.g., (None, height, width, channels)\n        if input_shape[1] is not None:\n            target_size = input_shape[1]\n        else:\n            target_size = img_size\n        # Determine color mode based on number of channels\n        if input_shape[-1] == 3:\n            color_mode = \"rgb\"\n        else:\n            color_mode = \"grayscale\"\n    \n    # Load and preprocess the image\n    img = load_img(img_path, target_size=(target_size, target_size), color_mode=color_mode)\n    img_array = img_to_array(img) / 255.0\n\n    # Reshape based on model type: flatten for dense inputs, add batch dimension for conv inputs.\n    if len(input_shape) == 2:\n        # Flatten for dense layer\n        img_array = img_array.reshape(1, -1)\n    else:\n        # Keep dimensions for conv layer\n        img_array = np.expand_dims(img_array, axis=0)\n    \n    # Make prediction\n    predictions = model.predict(img_array)\n    \n    # Handle different output formats (assume single probability output)\n    pred_prob = predictions[0][0]\n    if pred_prob < 0.5:\n        return \"Normal\", 1 - pred_prob\n    else:\n        return \"TB Detected\", pred_prob\n\n# Update the image path\nimg_path = \"/kaggle/input/dvgdgdggdgd/Normal-64.png\"\nif not os.path.isfile(img_path):\n    raise FileNotFoundError(f\"Image file not found: {img_path}\")\n\n# Run prediction\npred_class, confidence = predict_tb(model, img_path, img_size=24)\nprint(f\"Prediction: {pred_class} | Confidence: {confidence:.4f}\")\n\n# Display the image\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T23:03:53.148621Z","iopub.execute_input":"2025-03-29T23:03:53.148956Z","iopub.status.idle":"2025-03-29T23:03:53.615913Z","shell.execute_reply.started":"2025-03-29T23:03:53.148930Z","shell.execute_reply":"2025-03-29T23:03:53.615168Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 187ms/step\nPrediction: Normal | Confidence: 1.0000\n","output_type":"stream"}],"execution_count":22}]}